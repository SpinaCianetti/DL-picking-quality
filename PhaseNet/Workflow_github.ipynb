{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d724847-c29f-4d95-8705-af3ea837c6fb",
   "metadata": {},
   "source": [
    "## Notebook to compute ML picks for INGV events in Norcia Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffc659e-ecd5-44da-be41-bb27cee51fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.core.event import  Event, Origin, Magnitude, Pick, WaveformStreamID\n",
    "from obspy import Catalog\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from obspy import read_inventory, read_events\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.filesystem.sds import Client as sdsclient\n",
    "from obspy.clients.fdsn import RoutingClient\n",
    "from obspy.core import Trace, Stream, Stats\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b5a50-a91d-4bd8-888e-d2d684630764",
   "metadata": {},
   "source": [
    "### Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec7f61a-480c-4ffe-8e62-ad68fb77d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(starttime, endtime, inventory):\n",
    "    max_retry = 10\n",
    "    stream = obspy.Stream()\n",
    "    for network in inventory:\n",
    "        for station in network:\n",
    "            retry = 0\n",
    "            while retry < max_retry:\n",
    "                try:\n",
    "                    ch=inventory.select(station=station.code).get_contents()['channels'][0].split('.')[-1][:2]+\"?\"\n",
    "                    if network.code ==\"YR\":\n",
    "                        tmp = sdsYR.get_waveforms(\n",
    "                            network=network.code, \n",
    "                            station=station.code, \n",
    "                            location=\"\", \n",
    "                            channel=ch, \n",
    "                            starttime=starttime, \n",
    "                            endtime=endtime\n",
    "                        )\n",
    "                    else:\n",
    "                        tmp = sds.get_waveforms(\n",
    "                            network=network.code, \n",
    "                            station=station.code, \n",
    "                            location=\"\", \n",
    "                            channel=ch, \n",
    "                            starttime=starttime, \n",
    "                            endtime=endtime\n",
    "                        )\n",
    "                    if len(tmp) > 0:\n",
    "                        tmp.merge(method=0,fill_value=0)\n",
    "                    for trace in tmp:\n",
    "                        if trace.stats.sampling_rate != 100:\n",
    "                            trace = trace.interpolate(100, method=\"linear\")\n",
    "                    stream += tmp\n",
    "                    \n",
    "                    break\n",
    "                except Exception as err:\n",
    "                    print(\"Error {}.{}: {}\".format(network.code, station.code, err))\n",
    "                    message = \"No data available for request.\"\n",
    "                    if str(err)[: len(message)] == message:\n",
    "                        break\n",
    "                    retry += 1\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "            if retry == max_retry:\n",
    "                print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code}\")\n",
    "    stream.merge(method=0,fill_value=0)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90a0376-812c-42ab-bae1-1868ac917cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_zero_stream(network, station, channels, start_time, end_time, sampling_rate):\n",
    "    stream = Stream()\n",
    "\n",
    "    for channel in channels:\n",
    "        stats = Stats()\n",
    "        stats.network = network\n",
    "        stats.station = station\n",
    "        stats.channel = channel\n",
    "        stats.sampling_rate = sampling_rate\n",
    "        stats.starttime = UTCDateTime(start_time)\n",
    "\n",
    "        trace_data = [0.0] * int((end_time - start_time) * sampling_rate)\n",
    "        trace = Trace(data=np.array(trace_data), header=stats)\n",
    "\n",
    "        stream.append(trace)\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76c9cba-045d-476e-a33b-ce5943d3486b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def findpicks(pddataframe, picker,threshold,inventory):\n",
    "    name=\"pn\"\n",
    "    Plist = []\n",
    "    Slist = []\n",
    "    mseedlist = []\n",
    "    for i, row in pddataframe.iterrows():\n",
    "        sta = row['Station']\n",
    "        Pori = row['P pick']\n",
    "        Sori = row['S pick']\n",
    "        if pd.notna(Pori):\n",
    "            t0 = Pori - 20\n",
    "            t1 = Pori + 40\n",
    "        else:\n",
    "            t0 = Sori - 20\n",
    "            t1 = Sori + 40\n",
    "\n",
    "        inv = inventory.select(station=sta,starttime=t0, endtime=t1)\n",
    "        for net in inv:\n",
    "            if net.code == \"8P\":  # Mismatch between mseed and inventory\n",
    "                net.code = \"IV\"\n",
    "\n",
    "        mseed = download(t0, t1, inv)\n",
    "        if len(mseed) == 0:\n",
    "            channels=[]\n",
    "            for net in inv:\n",
    "                network = net.code\n",
    "                for sta in net:\n",
    "                    station = sta.code\n",
    "                    for ch in sta:\n",
    "                        channels.append(ch.code)\n",
    "                                               \n",
    "            start_time = t0\n",
    "            end_time = t1\n",
    "            sampling_rate = 100.0 \n",
    "\n",
    "            mseed = create_zero_stream(network, station, channels, start_time, end_time, sampling_rate)\n",
    "\n",
    "            \n",
    "        mseedlist.append(mseed[0].get_id())\n",
    "        for trace in mseed:\n",
    "            if trace.stats.sampling_rate != 100:\n",
    "                trace.resample(100.)\n",
    "\n",
    "        picks = picker.classify(mseed, overlap=2800, stacking='max', P_threshold=threshold, S_threshold=threshold).picks\n",
    "        # picks = picker.classify(mseed, P_threshold=threshold, S_threshold=threshold).picks\n",
    "        deltap = 1e30\n",
    "        deltas = 1e30\n",
    "        PP = ''\n",
    "        if pd.notna(Pori):  # Check if Pori is not NaN\n",
    "            for p in picks:\n",
    "                if p.phase == 'P':\n",
    "                    if abs(Pori - obspy.UTCDateTime(p.peak_time)) < deltap:\n",
    "                        PP = obspy.UTCDateTime(p.peak_time)\n",
    "                        deltap = abs(Pori - PP)\n",
    "        # Plist.append(PP)\n",
    "            Plist.append(PP)\n",
    "        else:\n",
    "            Plist.append('')\n",
    "\n",
    "        if pd.notna(Sori):  # Check if Sori is not NaN\n",
    "            SS = ''\n",
    "            for p in picks:\n",
    "                if p.phase == 'S':\n",
    "                    if abs(Sori - obspy.UTCDateTime(p.peak_time)) < deltas:\n",
    "                        SS = obspy.UTCDateTime(p.peak_time)\n",
    "                        deltas = abs(Sori - SS)\n",
    "            Slist.append(SS)\n",
    "        else:\n",
    "            Slist.append('')\n",
    "\n",
    "    namep = \"P \" + name\n",
    "    names = \"S \" + name\n",
    "    pddataframe['id'] = mseedlist\n",
    "    # pddataframe['Dist'] = distlist\n",
    "    pddataframe[namep] = Plist\n",
    "    pddataframe[names] = Slist\n",
    "    return pddataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baaf8556-7259-45c5-a4db-ee2603aba6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_event(df_local, origintime, eve_id, savelocaldirectory):\n",
    "    # print(origintime,test)\n",
    "    timedelta = 2\n",
    "\n",
    "    obsdir = \"/home/jovyan/shared/users/spina/Norcia/github/OBS/\"+savelocaldirectory+\"/\" #Your local directory to save the files for NLLoc\n",
    "\n",
    "    if not os.path.exists(obsdir):\n",
    "        os.makedirs(obsdir)\n",
    "\n",
    "    cat = Catalog()\n",
    "    cat.description = \"Norcia_test\"\n",
    "    phaselist = ['P', 'S']\n",
    "\n",
    "    e = Event()\n",
    "    e.event_type = \"Earthquake\"\n",
    "    e.resource_id = eve_id\n",
    "    o = Origin()\n",
    "    o.time = origintime\n",
    "\n",
    "    for i, row in df_local.iterrows():\n",
    "        if pd.notna(row['P pick']) and (row['P pn'] != ''):\n",
    "            if abs(row['P pick'] - row['P pn']) <= timedelta:\n",
    "                wav_id = WaveformStreamID(\n",
    "                    station_code=row['Station'],\n",
    "                    channel_code=\"Z\",\n",
    "                    network_code=row['id'].split('.')[0]\n",
    "                )\n",
    "                e.picks.append(Pick(\n",
    "                    time=row['P pn'],\n",
    "                    waveform_id=wav_id,\n",
    "                    phase_hint='P',\n",
    "                    evaluation_mode=\"automatic\",\n",
    "                    time_errors=0.02\n",
    "                ))\n",
    "\n",
    "        if pd.notna(row['S pick']) and (row['S pn'] != ''):\n",
    "            if abs(row['S pick'] - row['S pn']) <= timedelta:\n",
    "                wav_id = WaveformStreamID(\n",
    "                    station_code=row['Station'],\n",
    "                    channel_code=\"N\",\n",
    "                    network_code=row['id'].split('.')[0]\n",
    "                )\n",
    "                e.picks.append(Pick(\n",
    "                    time=row['S pn'],\n",
    "                    waveform_id=wav_id,\n",
    "                    phase_hint='S',\n",
    "                    evaluation_mode=\"automatic\",\n",
    "                    time_errors=0.04\n",
    "                ))\n",
    "\n",
    "    # print(e.picks)\n",
    "    if len(e.picks) > 0:\n",
    "        fileOBS = obsdir + \"Norcia_test_\" + str(origintime) + \"_\" + \".phs\"\n",
    "        e.write(fileOBS, format=\"NLLOC_OBS\")\n",
    "\n",
    "        with open(fileOBS, \"r+\") as f: s = f.read(); f.seek(0); f.write(\"PUBLIC_ID \"+str(e.resource_id)+\"\\n\" + s)\n",
    "    else:\n",
    "        print('No picks for event',e.resource_id)\n",
    "        f = open('no_event.txt', 'a')\n",
    "        f.write(str(e.resource_id)+\"\\n\")\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ec1f55-98b9-460f-a660-083b9400e3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "client = Client(\"INGV\")\n",
    "sds=sdsclient(\"/home/jovyan/data/sds/\")\n",
    "sdsYR=sdsclient(\"/home/jovyan/data/iris/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b1938b-b758-4382-9258-868ed131b337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-20T00:00:00.000000Z 2016-10-21T00:00:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "starttime=UTCDateTime(\"2016-10-20T00:00:00\")\n",
    "endtime=UTCDateTime(\"2016-10-21T00:00:00\")\n",
    "\n",
    "print(starttime,endtime)\n",
    "nday=int((endtime-starttime)/86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de88d2b7-27da-4e1c-a44c-3726c32c3e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "center = (13.1, 42.825)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d214518-8dd8-4265-9af7-d37543856b5c",
   "metadata": {},
   "source": [
    "## Read INGV Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936e2e12-8fc6-4853-948a-eed571a7bb1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catINGV = read_events(\"./catalog_ingv.xml\")\n",
    "catINGV = Catalog(sorted(catINGV, key=lambda e: e.origins[0].time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22dc9e-9e98-44f8-8750-b2b355fc7579",
   "metadata": {},
   "source": [
    "## Read Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be6b332-d1bd-43ff-8988-e06ea8983373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INVE='./INVENTORY/*.xml'\n",
    "\n",
    "ii  = glob.glob(INVE)\n",
    "inventory=obspy.Inventory()\n",
    "\n",
    "for e in ii:\n",
    "    inventory+=read_inventory(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "219c2372-72fe-4c87-963b-94b0e9718fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stalist=set()\n",
    "for net in inventory.select(channel=\"*Z\"):\n",
    "    for sta in net:\n",
    "        stalist.add(sta.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060ac5a-ccb4-4c0c-81af-f46f3226d692",
   "metadata": {},
   "source": [
    "### Load Seisbench model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23a6e02-06d8-4a04-aea8-00de35b419be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/seisbench/models/base.py:489: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load(f\"{path_pt}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PhaseNet model from Zhu et al. (2018). Originally published under MIT License. Original available at https://github.com/AI4EPS/PhaseNet/tree/master/model/190703-214543 . \n",
      "\n",
      "Converted to SeisBench by Jannes MÃ¼nchmeyer (munchmej@univ-grenoble-alpes.fr) with help from Sacha Lapins, Yiyuan Zhong, and Jun Zhu\n"
     ]
    }
   ],
   "source": [
    "import seisbench.models as sbm\n",
    "\n",
    "picker_pno = sbm.PhaseNet.from_pretrained(\"original\")\n",
    "picker_pni = sbm.PhaseNet.from_pretrained(\"instance\")\n",
    "\n",
    "picker_pno.cuda()\n",
    "picker_pni.cuda()\n",
    "print(picker_pno.weights_docstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2a7df-9e4c-4891-81f6-e1eba8c3d471",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract picks from INGV catalog correspondig to arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011d0f77-0ada-49b9-8642-5354438d6733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for curreve in catINGV:\n",
    "    staall = set([])\n",
    "    ingvpick = []\n",
    "    data_dict = {}\n",
    "    event_id_str = str(curreve.resource_id)\n",
    "    evento_id = event_id_str.split(\"eventId=\")[-1]\n",
    "    ori = curreve.origins[0]\n",
    "    arrivals = ori.arrivals\n",
    "    picks=curreve.picks\n",
    "    t = ori.time\n",
    "    lon = ori.longitude\n",
    "    lat = ori.latitude\n",
    "    dep = ori.depth\n",
    "    inventory=read_inventory('./INVENTORY/inventory_ingv'+str(t.julday)+\".xml\")\n",
    "    stalist=[]\n",
    "    for net in inventory.select(channel=\"*Z\"):\n",
    "        for sta in net:\n",
    "            stalist.append(sta.code)\n",
    "    for ar in arrivals:\n",
    "        if (ar.phase in ['P','S','Pn','Sn','Pg','Sg']):# and ar.time_weight >= 0.00001):\n",
    "            pi = [p for p in picks if p.resource_id == ar.pick_id][0]\n",
    "#             print(pi.waveform_id.station_code,pi.waveform_id.channel_code,pi.phase_hint)\n",
    "            sta = pi.waveform_id.station_code\n",
    "            staall.add(sta)\n",
    "            if sta in stalist:\n",
    "                ingvpick.append(pi)\n",
    "\n",
    "    # Initialize the 'S pick' column with np.nan in the dictionary\n",
    "    for pick in ingvpick:\n",
    "        station_code = pick.waveform_id.station_code\n",
    "        pick_time = pick.time\n",
    "\n",
    "        # Check phase_hint and update the dictionary accordingly\n",
    "        if (pick.phase_hint == 'P') or (pick.phase_hint == 'Pg') or (pick.phase_hint == 'Pn'):\n",
    "            if station_code not in data_dict:\n",
    "                data_dict[station_code] = {'Station': station_code, 'P pick': pick_time, 'S pick': np.nan}\n",
    "            else:\n",
    "                data_dict[station_code]['P pick'] = pick_time\n",
    "        elif (pick.phase_hint == 'S') or (pick.phase_hint == 'Sg') or (pick.phase_hint == 'Sn'):\n",
    "            if station_code not in data_dict:\n",
    "                data_dict[station_code] = {'Station': station_code, 'P pick': np.nan, 'S pick': pick_time}\n",
    "            else:\n",
    "                data_dict[station_code]['S pick'] = pick_time\n",
    "\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries and create a DataFrame\n",
    "    df_picks = pd.DataFrame(list(data_dict.values()))\n",
    "    # print(df_picks)\n",
    "\n",
    "## Define model a\n",
    "    picker=picker_pni\n",
    "    thresholds=0.05\n",
    "    dirsave='PN_IN28_005'\n",
    "# Now call phasenet to find the picks         \n",
    "    dfn=findpicks(df_picks,picker,thresholds,inventory)\n",
    "\n",
    "    write_event(dfn, ori.time, evento_id, dirsave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3bf809-74c7-44fc-b1b9-911652d9561d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>P pick</th>\n",
       "      <th>S pick</th>\n",
       "      <th>id</th>\n",
       "      <th>P pn</th>\n",
       "      <th>S pn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARVD</td>\n",
       "      <td>2016-10-20T00:16:57.090000Z</td>\n",
       "      <td>2016-10-20T00:17:06.990000Z</td>\n",
       "      <td>IV.ARVD..HHE</td>\n",
       "      <td>2016-10-20T00:16:56.240000Z</td>\n",
       "      <td>2016-10-20T00:17:06.210000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1211</td>\n",
       "      <td>2016-10-20T00:16:54.190000Z</td>\n",
       "      <td>2016-10-20T00:17:02.190000Z</td>\n",
       "      <td>IV.T1211..EHE</td>\n",
       "      <td>2016-10-20T00:16:54.090000Z</td>\n",
       "      <td>2016-10-20T00:17:02.180000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RM33</td>\n",
       "      <td>2016-10-20T00:16:52.900000Z</td>\n",
       "      <td>2016-10-20T00:17:00.320000Z</td>\n",
       "      <td>IV.RM33..EHE</td>\n",
       "      <td>2016-10-20T00:16:52.850000Z</td>\n",
       "      <td>2016-10-20T00:17:00.250000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TERO</td>\n",
       "      <td>2016-10-20T00:16:52.660000Z</td>\n",
       "      <td>2016-10-20T00:16:59.410000Z</td>\n",
       "      <td>IV.TERO..HHE</td>\n",
       "      <td>2016-10-20T00:16:52.460000Z</td>\n",
       "      <td>2016-10-20T00:16:59.310000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1217</td>\n",
       "      <td>2016-10-20T00:16:51.160000Z</td>\n",
       "      <td>2016-10-20T00:16:56.650000Z</td>\n",
       "      <td>IV.T1217..EHE</td>\n",
       "      <td>2016-10-20T00:16:51.110000Z</td>\n",
       "      <td>2016-10-20T00:16:56.700000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T1215</td>\n",
       "      <td>2016-10-20T00:16:50.840000Z</td>\n",
       "      <td>2016-10-20T00:16:56.100000Z</td>\n",
       "      <td>IV.T1215..EHE</td>\n",
       "      <td>2016-10-20T00:16:50.790000Z</td>\n",
       "      <td>2016-10-20T00:16:55.760000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMA1</td>\n",
       "      <td>2016-10-20T00:16:50.890000Z</td>\n",
       "      <td>2016-10-20T00:16:56.000000Z</td>\n",
       "      <td>IV.SMA1..EHE</td>\n",
       "      <td>2016-10-20T00:16:50.850000Z</td>\n",
       "      <td>2016-10-20T00:16:56.020000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARRO</td>\n",
       "      <td>2016-10-20T00:16:54.480000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.ARRO..EHE</td>\n",
       "      <td>2016-10-20T00:16:54.140000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CESI</td>\n",
       "      <td>2016-10-20T00:16:50.070000Z</td>\n",
       "      <td>2016-10-20T00:16:54.580000Z</td>\n",
       "      <td>IV.CESI..HHE</td>\n",
       "      <td>2016-10-20T00:16:50.010000Z</td>\n",
       "      <td>2016-10-20T00:16:54.550000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATCC</td>\n",
       "      <td>2016-10-20T00:16:54.540000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.ATCC..EHE</td>\n",
       "      <td>2016-10-20T00:16:54.490000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CING</td>\n",
       "      <td>2016-10-20T00:16:53.530000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.CING..HHE</td>\n",
       "      <td>2016-10-20T00:16:53.480000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SNTG</td>\n",
       "      <td>2016-10-20T00:16:52.580000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.SNTG..HHE</td>\n",
       "      <td>2016-10-20T00:16:52.200000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OFFI</td>\n",
       "      <td>2016-10-20T00:16:52.260000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.OFFI..HHE</td>\n",
       "      <td>2016-10-20T00:16:52.260000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ASSB</td>\n",
       "      <td>2016-10-20T00:16:51.270000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.ASSB..HHE</td>\n",
       "      <td>2016-10-20T00:16:53.170000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LNSS</td>\n",
       "      <td>2016-10-20T00:16:51.940000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.LNSS..HHE</td>\n",
       "      <td>2016-10-20T00:16:51.930000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GUMA</td>\n",
       "      <td>2016-10-20T00:16:48.150000Z</td>\n",
       "      <td>2016-10-20T00:16:51.020000Z</td>\n",
       "      <td>IV.GUMA..HHE</td>\n",
       "      <td>2016-10-20T00:16:48.130000Z</td>\n",
       "      <td>2016-10-20T00:16:51.020000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T1243</td>\n",
       "      <td>2016-10-20T00:16:50.540000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.T1243..HHE</td>\n",
       "      <td>2016-10-20T00:16:50.480000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T1202</td>\n",
       "      <td>2016-10-20T00:16:49.740000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.T1202..EHE</td>\n",
       "      <td>2016-10-20T00:16:49.590000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AM05</td>\n",
       "      <td>2016-10-20T00:16:46.940000Z</td>\n",
       "      <td>2016-10-20T00:16:48.550000Z</td>\n",
       "      <td>XO.AM05..EHE</td>\n",
       "      <td>2016-10-20T00:16:46.910000Z</td>\n",
       "      <td>2016-10-20T00:16:48.560000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MMO1</td>\n",
       "      <td>2016-10-20T00:16:46.500000Z</td>\n",
       "      <td>2016-10-20T00:16:47.840000Z</td>\n",
       "      <td>IV.MMO1..EHE</td>\n",
       "      <td>2016-10-20T00:16:46.490000Z</td>\n",
       "      <td>2016-10-20T00:16:47.930000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>T1216</td>\n",
       "      <td>2016-10-20T00:16:47.620000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.T1216..EHE</td>\n",
       "      <td>2016-10-20T00:16:48.400000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MC2</td>\n",
       "      <td>2016-10-20T00:16:46.260000Z</td>\n",
       "      <td>2016-10-20T00:16:47.450000Z</td>\n",
       "      <td>IV.MC2..EHE</td>\n",
       "      <td>2016-10-20T00:16:46.310000Z</td>\n",
       "      <td>2016-10-20T00:16:47.530000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>T1245</td>\n",
       "      <td>2016-10-20T00:16:46.200000Z</td>\n",
       "      <td>2016-10-20T00:16:46.670000Z</td>\n",
       "      <td>IV.T1245..HHE</td>\n",
       "      <td>2016-10-20T00:16:46.180000Z</td>\n",
       "      <td>2016-10-20T00:16:46.790000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NRCA</td>\n",
       "      <td>2016-10-20T00:16:46.810000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IV.NRCA..HHE</td>\n",
       "      <td>2016-10-20T00:16:46.790000Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Station                       P pick                       S pick  \\\n",
       "0     ARVD  2016-10-20T00:16:57.090000Z  2016-10-20T00:17:06.990000Z   \n",
       "1    T1211  2016-10-20T00:16:54.190000Z  2016-10-20T00:17:02.190000Z   \n",
       "2     RM33  2016-10-20T00:16:52.900000Z  2016-10-20T00:17:00.320000Z   \n",
       "3     TERO  2016-10-20T00:16:52.660000Z  2016-10-20T00:16:59.410000Z   \n",
       "4    T1217  2016-10-20T00:16:51.160000Z  2016-10-20T00:16:56.650000Z   \n",
       "5    T1215  2016-10-20T00:16:50.840000Z  2016-10-20T00:16:56.100000Z   \n",
       "6     SMA1  2016-10-20T00:16:50.890000Z  2016-10-20T00:16:56.000000Z   \n",
       "7     ARRO  2016-10-20T00:16:54.480000Z                          NaN   \n",
       "8     CESI  2016-10-20T00:16:50.070000Z  2016-10-20T00:16:54.580000Z   \n",
       "9     ATCC  2016-10-20T00:16:54.540000Z                          NaN   \n",
       "10    CING  2016-10-20T00:16:53.530000Z                          NaN   \n",
       "11    SNTG  2016-10-20T00:16:52.580000Z                          NaN   \n",
       "12    OFFI  2016-10-20T00:16:52.260000Z                          NaN   \n",
       "13    ASSB  2016-10-20T00:16:51.270000Z                          NaN   \n",
       "14    LNSS  2016-10-20T00:16:51.940000Z                          NaN   \n",
       "15    GUMA  2016-10-20T00:16:48.150000Z  2016-10-20T00:16:51.020000Z   \n",
       "16   T1243  2016-10-20T00:16:50.540000Z                          NaN   \n",
       "17   T1202  2016-10-20T00:16:49.740000Z                          NaN   \n",
       "18    AM05  2016-10-20T00:16:46.940000Z  2016-10-20T00:16:48.550000Z   \n",
       "19    MMO1  2016-10-20T00:16:46.500000Z  2016-10-20T00:16:47.840000Z   \n",
       "20   T1216  2016-10-20T00:16:47.620000Z                          NaN   \n",
       "21     MC2  2016-10-20T00:16:46.260000Z  2016-10-20T00:16:47.450000Z   \n",
       "22   T1245  2016-10-20T00:16:46.200000Z  2016-10-20T00:16:46.670000Z   \n",
       "23    NRCA  2016-10-20T00:16:46.810000Z                          NaN   \n",
       "\n",
       "               id                         P pn                         S pn  \n",
       "0    IV.ARVD..HHE  2016-10-20T00:16:56.240000Z  2016-10-20T00:17:06.210000Z  \n",
       "1   IV.T1211..EHE  2016-10-20T00:16:54.090000Z  2016-10-20T00:17:02.180000Z  \n",
       "2    IV.RM33..EHE  2016-10-20T00:16:52.850000Z  2016-10-20T00:17:00.250000Z  \n",
       "3    IV.TERO..HHE  2016-10-20T00:16:52.460000Z  2016-10-20T00:16:59.310000Z  \n",
       "4   IV.T1217..EHE  2016-10-20T00:16:51.110000Z  2016-10-20T00:16:56.700000Z  \n",
       "5   IV.T1215..EHE  2016-10-20T00:16:50.790000Z  2016-10-20T00:16:55.760000Z  \n",
       "6    IV.SMA1..EHE  2016-10-20T00:16:50.850000Z  2016-10-20T00:16:56.020000Z  \n",
       "7    IV.ARRO..EHE  2016-10-20T00:16:54.140000Z                               \n",
       "8    IV.CESI..HHE  2016-10-20T00:16:50.010000Z  2016-10-20T00:16:54.550000Z  \n",
       "9    IV.ATCC..EHE  2016-10-20T00:16:54.490000Z                               \n",
       "10   IV.CING..HHE  2016-10-20T00:16:53.480000Z                               \n",
       "11   IV.SNTG..HHE  2016-10-20T00:16:52.200000Z                               \n",
       "12   IV.OFFI..HHE  2016-10-20T00:16:52.260000Z                               \n",
       "13   IV.ASSB..HHE  2016-10-20T00:16:53.170000Z                               \n",
       "14   IV.LNSS..HHE  2016-10-20T00:16:51.930000Z                               \n",
       "15   IV.GUMA..HHE  2016-10-20T00:16:48.130000Z  2016-10-20T00:16:51.020000Z  \n",
       "16  IV.T1243..HHE  2016-10-20T00:16:50.480000Z                               \n",
       "17  IV.T1202..EHE  2016-10-20T00:16:49.590000Z                               \n",
       "18   XO.AM05..EHE  2016-10-20T00:16:46.910000Z  2016-10-20T00:16:48.560000Z  \n",
       "19   IV.MMO1..EHE  2016-10-20T00:16:46.490000Z  2016-10-20T00:16:47.930000Z  \n",
       "20  IV.T1216..EHE  2016-10-20T00:16:48.400000Z                               \n",
       "21    IV.MC2..EHE  2016-10-20T00:16:46.310000Z  2016-10-20T00:16:47.530000Z  \n",
       "22  IV.T1245..HHE  2016-10-20T00:16:46.180000Z  2016-10-20T00:16:46.790000Z  \n",
       "23   IV.NRCA..HHE  2016-10-20T00:16:46.790000Z                               "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a8318-aa7b-4c90-b4cf-df1c0eb170e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
